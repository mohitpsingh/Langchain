{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjaZ0slPbhg6iwdXUamMru",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohitpsingh/Langchain/blob/feature%2Fchatbot_tutorial/Langchain_Tutorial8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resuable Prompt Template where we define the one Dynamic Variable which changes the Prompt"
      ],
      "metadata": {
        "id": "xC5iB6IT5LbJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqfZamOsYTwe",
        "outputId": "e54ce7b9-2539-4f22-e3b4-84252ffb0c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "1Hwszv2FcPfY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = (\n",
        "    PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "    + \", make it funny\"\n",
        "    + \"\\n\\n and in {language}\"\n",
        ")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvud8T8Acg8v",
        "outputId": "bc02415c-1b7b-400d-c13c-238520a244a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['language', 'topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}, make it funny\\n\\n and in {language}')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalPrompt = prompt.format(topic=\"Social\", language=\"hindi\")"
      ],
      "metadata": {
        "id": "TKWkp2mRdlb9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat_model = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0.6\n",
        ")\n",
        "\n",
        "chat_model.invoke(finalPrompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZOIcC_uesdq",
        "outputId": "2012b248-8c8b-4175-9d78-005ff3d3b44a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Sure, here\\'s a joke in Hindi about social media:\\n\\n\"Do you know why Facebook is always cold?\\n\\nBecause it has a lot of \\'friend requests\\'!\"\\n\\nTranslation: \"Aapko pata hai kyon Facebook thandi rehti hai? Kyonki usme bahut saare \\'dost ke prarthana\\' hote rehte hain!\"\\n\\nI hope you found this joke amusing!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 23, 'total_tokens': 126, 'completion_time': 0.158085703, 'prompt_time': 0.00261072, 'queue_time': 0.035256920000000004, 'total_time': 0.160696423}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-207ef437-af1a-45f1-975d-8be4b53ae89b-0', usage_metadata={'input_tokens': 23, 'output_tokens': 103, 'total_tokens': 126})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "prompt = SystemMessage(content=\"You are a Comedian\")"
      ],
      "metadata": {
        "id": "KEfDhpmTeWWr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = (\n",
        "    prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"Yes, How can I help you?\") + \"{input}\"\n",
        ")"
      ],
      "metadata": {
        "id": "5JFkvXR8gbc0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt.format_messages(input=\"Tell me a joke about Social Media\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hix9EksxguMD",
        "outputId": "8a570dcd-4f0d-4de4-fe8e-afc84ee0fdde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a Comedian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Yes, How can I help you?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Tell me a joke about Social Media', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate\n",
        "\n",
        "full_template = \"\"\"{intrduction}\n",
        "\n",
        "{example}\n",
        "\n",
        "{start}\"\"\"\n",
        "full_prompt = PromptTemplate.from_template(full_template)\n",
        "\n",
        "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
        "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
        "\n",
        "example_template = \"\"\"Here's an example of an interaction:\n",
        "\n",
        "Q: {example_q}\n",
        "A: {example_a}\"\"\"\n",
        "example_prompt = PromptTemplate.from_template(example_template)\n",
        "\n",
        "start_template = \"\"\"Now, do this for real!\n",
        "\n",
        "Q: {input}\n",
        "A:\"\"\"\n",
        "start_prompt = PromptTemplate.from_template(start_template)\n",
        "\n",
        "input_prompt = [\n",
        "    (\"intrduction\", introduction_prompt),\n",
        "    (\"example\", example_prompt),\n",
        "    (\"start\", start_prompt)\n",
        "]\n",
        "\n",
        "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompt)\n",
        "\n",
        "pipeline_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PirGZLyyuImN",
        "outputId": "61cba3f4-ca90-484b-c4a5-e3bf21c2875c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['person', 'input', 'example_q', 'example_a']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    pipeline_prompt.format(\n",
        "        person=\"Elon Musk\",\n",
        "        example_q=\"What's your favorite car?\",\n",
        "        example_a=\"Tesla\",\n",
        "        input=\"What's your favorite social media site?\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW66j3BjwzwM",
        "outputId": "7f27e26d-fee2-496b-9472-bfc68eff8377"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are impersonating Elon Musk.\n",
            "\n",
            "Here's an example of an interaction:\n",
            "\n",
            "Q: What's your favorite car?\n",
            "A: Tesla\n",
            "\n",
            "Now, do this for real!\n",
            "\n",
            "Q: What's your favorite social media site?\n",
            "A:\n"
          ]
        }
      ]
    }
  ]
}