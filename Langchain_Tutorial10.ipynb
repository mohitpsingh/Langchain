{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIfLjpO9JQV/3C/6JGn3dl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohitpsingh/Langchain/blob/feature%2Fchatbot_tutorial/Langchain_Tutorial10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## cache LLM responses"
      ],
      "metadata": {
        "id": "8ukM3fnH38pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "drpvCGRa2bSX"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "kz_cbInE2pOX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm_model = ChatGroq(model=\"mixtral-8x7b-32768\")"
      ],
      "metadata": {
        "id": "it7tSpZw2rXk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "llm_model.invoke(\"Tell me a Joke!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoYtyCfl3GsI",
        "outputId": "e6528916-83f1-49f9-f402-c2c5e32be760"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25.6 ms, sys: 3.8 ms, total: 29.4 ms\n",
            "Wall time: 289 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure,\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\n(I'm assuming you're okay with clean jokes, if not let me know and I'll adjust accordingly)\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 13, 'total_tokens': 61, 'completion_time': 0.072941237, 'prompt_time': 0.002057575, 'queue_time': 0.034307523, 'total_time': 0.074998812}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad6e70d0-c01b-4d1a-bbde-e249d8411f35-0', usage_metadata={'input_tokens': 13, 'output_tokens': 48, 'total_tokens': 61})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "llm_model.invoke(\"Tell me a Joke!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbiOAtVm3u-B",
        "outputId": "43c40375-2208-4a5b-b446-a8f203039cae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.8 ms, sys: 0 ns, total: 1.8 ms\n",
            "Wall time: 1.68 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure,\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\n(I'm assuming you're okay with clean jokes, if not let me know and I'll adjust accordingly)\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 13, 'total_tokens': 61, 'completion_time': 0.072941237, 'prompt_time': 0.002057575, 'queue_time': 0.034307523, 'total_time': 0.074998812}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad6e70d0-c01b-4d1a-bbde-e249d8411f35-0', usage_metadata={'input_tokens': 13, 'output_tokens': 48, 'total_tokens': 61})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm .langchain.db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiGpbjPH8aux",
        "outputId": "0343112c-33a6-4dba-8c38-3f0a53195f44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '.langchain.db': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cache import SQLiteCache\n",
        "\n",
        "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
      ],
      "metadata": {
        "id": "KYZmyrLH8_2E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "llm_model.invoke(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9VLoxcL9e6g",
        "outputId": "80c3105a-58c2-4676-92dd-abe62ed1f665"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32.7 ms, sys: 2.83 ms, total: 35.5 ms\n",
            "Wall time: 276 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure,**Joke Request**\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.03889194, 'prompt_time': 0.001884377, 'queue_time': 0.036325402, 'total_time': 0.040776317}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-c18a04ae-64fd-488d-8784-06236e123f3b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "llm_model.invoke(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxhZNsom9lEc",
        "outputId": "18bace78-4b6a-4ea0-f887-bb91221b2096"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.73 ms, sys: 0 ns, total: 3.73 ms\n",
            "Wall time: 3.73 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure,**Joke Request**\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.03889194, 'prompt_time': 0.001884377, 'queue_time': 0.036325402, 'total_time': 0.040776317}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-c18a04ae-64fd-488d-8784-06236e123f3b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}